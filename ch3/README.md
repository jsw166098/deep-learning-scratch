# 3. 신경망

신경망은 가중치 매개변수으 적절한 값을 데이터로 부터 자동으로 학습하는 역할을 한다. 
신경망이 입력을 식별하는 처리 과정을 살펴볼 수 있다. 

## 3.1 퍼셉트론에서 신경망으로

### 3.1.1 신경망의 예

신경망은 입력층, 은닉층, 출력층으로 구성된다. 여기서 은닉충의 뉴런, 노드는 사람눈에 보이지 않는다.

그림에선는 입력층 0, 은닉층 1, 출력층 2층으로 구성된다.

뉴런이 연결되는 방식은 퍼셉트론과 같다.

### 3.1.2 퍼셉트론 복습

퍼셉트론의 수식

```
y={
0 (b+w1x1+w2x2 <= 0),
1 (b+w1x1+w2x2 > 0)
}
```

* b는 편향을 나타내는 매개변수로 얼마나 쉽게 활성화되느냐를 제어한다.
* w1, w2는 가중치를 나태니며 신호의 영향력을 제어한다.

편향을 명시한 퍼셉트론
![dlm_3_3](https://github.com/jsw166098/deep-learning-scratch/blob/main/img/dlsimg_3_2.png)

기존의 퍼셉트론 수식을 2단계로 나누어 표한한다. 

```
y = h(b+w1x1+w2x2)

h(x) = {
0 (x <= 0),
1 (x > 0)
}
```

* 첫번째 단계는 입력 신호의 총합이 h(x)라는 함수를 거쳐 변환되어, 그 변환된 값이 y의 출력이된다.
* 두번째 단계의 h(x) 함수는 입력이 0을 넘으면 1을 그렇지 않으면 0을 출력하게 된다.
* 첫번째 단계에서 입력 신호의 __총합을 계산__ 하고 두번째 단계의 __활성화 함수에 입력해 결과__ 를 낸다.

### 3.1.3 활성화 함수의 등장

#### 활성화 함수
* 입력 신호의 총합을 출력 신호로 변환 하는 함수
* 입력 신호의 총합이 활성화를 일으키는지 결정한다.

가중치가 달린 입력 신호와 편향의 총합을 계산하여 이를 a로 명시하면 다음과 같이 활성화 함수를 나태낼 수 있다.
뉴런, 노드에 활성화 처리 과정을 명시한 경우와 그렇지 않은 경우로 나누어 나타낼 수 있다.
![dlmImg_3_4]

일반적인 뉴런
![dlmImg_3_5_1]

활성화 처리 과정을 명시한 뉴런
![dlmImg_3_5_2]

```
퍼셉트론을 나타내는 방식으로 다음과 같은 것들이 있었다.
1. 편향을 명시한 퍼셉트론
2. 활성화 과정을 명시한 퍼셉트론
```

## 3.2 활성화 함수

#### 활성화 함수

* 입력 신호의 총합을 출력 신호로 변환 하는 함수
* 입력 신호의 총합을 구해 활성화 시킬것인지 안 그럴것인지 결정하는 함수 h(x)

#### 계단 함수

* 활성화 함수 중에서 임계값을 경계로 출력이 바뀌는 함수
* 퍼셉트론은 활성화 함수 중에서 계단 함수를 골라 사용한다. 
* 신경망은 활성화 함수를 계단 함수에서 다른 함수로 변형하는 형태로 구현된다.

### 3.2.1 시그모이드 함수

* 신경망에서 활성화 함수로 사용되는 함수이다.
* 입력의 총합을 계산하여 출력 신호를 계산한 후 다은 뉴런에 전달한다.

### 3.2.2 계단 함수 구현하기

* 입력이 0을 넘으면 1을 출력하고, 그 외에는 0을 출력한다.

인수가 실수인 계단 함수
```
def step_function(x):
    if x>0:
        return 1
    else:
        return 0
```

인수가 넘파이 배열인 계단 함수
~~~
def step_function():
    y = x>0  # array([False, True, True])
    return y.astype(np.int)  # 넘파이 배열 자료형 변환
~~~

### 3.2.3 계단 함수 그래프
```
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x>0, dtype = np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```
그래프의 모양이 계단과 같다고 해서 계단 함수라고 한다.
![dlm_3_5]()

### 3.2.4 시그모이드 함수 구현하기
~~~
def sigmoid(x):
    return 1/(1+np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
~~~

![dlm_3_6]()

### 3.2.5 시그모이드 함수와 계단 함수 비교

#### 공통점
* 두 함수는 모두 같은 모양을 가지고 있다. 
* 입력 신호가 작으면 0에, 크면 1에 가까운 값을 가진다. -> 입력이 중요하지 않으면 작은 값을 중요하면 큰 값을 출력한다.
* 범위는 0과 1 사이를 가진다. 
* 둘 다 비선형 함수이다.

#### 시그모이드 함수
* 시그모이드 함수는 매끄러운 곡선 형태이다. 
* 연속성을 띈다.
* 실수 형태로 연속적인 값을 가진다.

#### 계단 함수
* 계단 함수의 경우 직선형태이며 0을 기준으로 값이 갑자기 바뀐다.
* 0 또는 1로 2개의 값 만을 가진다.

### 3.2.6 비선형 함수

시그모이드 함수는 곡선, 계단 함수는 구부러진 직선 형태로 두 다 비선형 함수이다.

#### 선형 함수
* 선형 함수는 y=ax+b의 직선 형태로 나타난다. 
* a, b 모두 상수이기 때문에 출력값이 입력값의 상수배만큼 변하는 함수이다.

#### 비선형 함수
* 선형이 아닌 함수
* 직선 하나로 그릴수 없는 함수
* 신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다. -> 퍼셉트론에서 여러층을 사용하기 위해 -> 2개 이상의 층은 곡선형태를 띈다.

### ReLU 함수

입력이 0을 넘으면 그대로 출력하고 그렇지 않으면 0을 출력하는 활서화 함수

ReLU 함수 수식

~~~
h(x) = {
   x (x > 0),
   0 (x <= 0)
}
~~~

ReLU 함수 구현
```
def relu(x):
    return np.maximum(0,x)
```

## 3.3 다차원 배열의 계산

신경망을 효율적으로 궇녀하기 위해 넘파이의 다차원 배열을 사용한 계산법이 필요하다. 

### 3.3.1
~~~
import numpy as np
A = np.array([1,2,3,4])
print(A)
np.ndim(A)  #1, 배열의 차수
A.shape  #(4, ), 배열의 형상-> 튜블로 변환
A.shape[0]  #4, 
~~~

~~~
B = np.array([[1,2], [3,4], [5,6]])
print(B)
np.ndim(B)  #2
B.shape  #(3,2)
~~~

### 3.3.2 행렬의 내적(행렬 곱)

* 왼쪽 행렬의 행과 오른쪽 행렬의 열을 원소별로 곱하고 그 값들을 더해서 계산한다.
* +,- 와는 달리 피연산자의 순서가 다르면 결과도 달라진다. 
* np.dot() 은 넘파이 배열 2개를 인수로 받아 그 내적값을 반환하는 함수이다.

~~~
A = np.array([[1,2], [3,4]])
A.shape
B = np.array([[5,6], [7,8]])
B.shape
np.dot(A, B)
~~~

~~~
A = np.array([[1,2,3], [4,5,6]])
A.shape
B = np.array([[1,2], [3,4], [5,6]])
B.shape
np.dot(A, B)
~~~

#### 행렬의 곱에서 주의점

* 행렬에 곱에서는 행렬의 형상에 주의해야 한다.
* 첫 번째 행렬에서 열의 수와 두 번째 행렬에서 행수가 같아야 한다.
* 행렬 곱의 결과는 첫번째 행렬의 행, 두번째 행렬의 열수와 같아 진다. 
```
 A   B  =  C
3x2 2x4   3x4
```

### 3.3.3 신경망의 내적
~~~
X = np.array([1,2])
X.shape

W = np.array([[1,3,5], [2,4,6]])
print(W)

W.shape
Y = np.dot(X, W)
print(Y)
~~~

## 3.4 3층 신경망 구현하기

## 3.5 출력층 설계하기

* 신경망에서 출력층을 구현할 때는 분류, 회귀 두 문제에 따라 다른 활성화 함수가 사용된다.
* 분류-> 소프트맥스 함수, 회귀->항등함수
~~~
기계 학습은 분류+회귀로 나뉜다. 
분류: 데이터가 어느 클래스에 속하는지에 대한 문제이다.
회귀: 데이터에서 수치를 예측하는 문제이다.
~~~

### 3.5.1 항등 함수와 소프트 맥스 함수 구현하기

#### 항등 함수

* 입력 그대로 출력함수-> (항등 == 입력과 출력이 항상 같다)

#### 소프트맥스 함수

* k 번째 출력값은 해당 입력 신호의 지수 함수에서 모든 입력 신호에 대한 지수 함수의 합을 나눈 값이다.
* 지수 함수값 자체가 크기 때문에 오버플로우를 예방하기 위해 개선된 소프트맥스 함수를 사용하여 값을 계산한다. 
* 출력값은 0과 1사이의 값이다.
* 출력값의 총 합은 1이다.
* 넘파이 배열 안에서 대소별 크기의 위치는 변하지 않는다.
* 지수 함수의 계산에 따른 비용을 고려해 현업에서는 소프트맥스 함수를 사용하지 않는다. -> 확률 개념과 대소 관계에 따른 위치가 변하지 않기에 가장 큰 값을 결과값으로 한다.

##### 소프트맥스 함수와 확률
* 출력값의 범위가 0~1이며 총 합이 1이기 때문에 확률 개념을 적용할 수 있다.
* y[1] 값이 0.018일 경우 1.8% 의 확률을 가질 수 있다고 해석한다. 

~~~
기계 학습의 문제 풀이는 학습과 추론의 두 단계를 거친다. 
학습 단계에서 모델을 학습하고 추론 단계에서 학습한 모델로 미지의 데이터에 대해서 추론(분류)을 수행한다. 
~~~

### 3.5.4 출력층의 뉴런 수 정하기

* 출력층의 뉴런 수는 문제에 맞게 적절해 정한다.
* 일반적으로 분류에서 분류하고 싶은 클래스 수로 설정한다. 

## 3.6 손글씨 숫자 인식

### 3.6.1 MNIST 데이터셋

* 손글씨 숫자 이미지 집합
* 0~9 숫자 이미지 집합이다. 
* 훈련 이미지(60,000장)을 통해 학습하여 시험 이미지(10,000)을 얼마나 정확하게 분류하는지 평가










